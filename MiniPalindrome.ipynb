{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MiniPalindrome Experiment\n",
    "\n",
    "The MiniPalindrome dataset consists of 48 Java programs in eight classes, each solving the task of detecting whether all words in the input string are palindromes. The classes represent different strategies to solve the tasks, e.g. whether using custom methods or built-in methods.\n",
    "\n",
    "More information on the data set is available at [doi:10.4119/unibi/2900666](http://doi.org/10.4119/unibi/2900666)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "from edist.tree_utils import dataset_from_json\n",
    "dataset_name = 'minipalindrome'\n",
    "trees, filenames = dataset_from_json(dataset_name)\n",
    "\n",
    "# label the data according to filename\n",
    "X = []\n",
    "Y = []\n",
    "for i in range(len(trees)):\n",
    "    if(filenames[i].startswith('ImplAAA')):\n",
    "        Y.append(0)\n",
    "    elif(filenames[i].startswith('ImplAAB')):\n",
    "        Y.append(1)\n",
    "    elif(filenames[i].startswith('ImplABA')):\n",
    "        Y.append(2)\n",
    "    elif(filenames[i].startswith('ImplABB')):\n",
    "        Y.append(3)\n",
    "    elif(filenames[i].startswith('ImplBAA')):\n",
    "        Y.append(4)\n",
    "    elif(filenames[i].startswith('ImplBAB')):\n",
    "        Y.append(5)\n",
    "    elif(filenames[i].startswith('ImplBBA')):\n",
    "        Y.append(6)\n",
    "    elif(filenames[i].startswith('ImplBBB')):\n",
    "        Y.append(7)\n",
    "    else:\n",
    "        continue\n",
    "    X.append(trees[i])\n",
    "\n",
    "del trees\n",
    "del filenames\n",
    "\n",
    "import numpy as np\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute all pairwise tree edit distances\n",
    "import edist.multiprocess as mp\n",
    "import edist.ted as ted\n",
    "\n",
    "D = mp.pairwise_distances_symmetric(X, ted.standard_ted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show the distances\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots()\n",
    "cax = axs.matshow(D)\n",
    "fig.colorbar(cax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up possible C values for the SVM\n",
    "Cs     = [0.1, 1., 10., 100.]\n",
    "\n",
    "# pre-compute all kernel variations for all hyperparameter\n",
    "# values\n",
    "kernel_types = ['linear', 'RBF', 'ST', 'SST', 'PT']\n",
    "Ks = [[], [], [], [], []]\n",
    "\n",
    "# pre-compute linear kernel matrix via double centering\n",
    "import scipy.linalg\n",
    "d_row_mean = np.expand_dims(np.mean(D, axis=0), 1)\n",
    "K_linear = 0.5 * (-D + d_row_mean + d_row_mean.T - np.mean(D))\n",
    "Lambda, V = scipy.linalg.eig(K_linear)\n",
    "Lambda[Lambda < 0] = 0\n",
    "K_linear = np.real(np.dot(V, np.dot(np.diag(Lambda), V.T)))\n",
    "Ks[0] = [K_linear]\n",
    "del K_linear\n",
    "del Lambda\n",
    "del V\n",
    "\n",
    "# pre-compute RBF kernel matrix\n",
    "sigmas = np.array([0.1, 0.5, 1., 2.]) * np.mean(D)\n",
    "for s in range(len(sigmas)):\n",
    "    K = np.exp(-0.5 * np.square(D) / (sigmas[s] ** 2))\n",
    "    K = 0.5 * (K + K.T)\n",
    "    Lambda, V = scipy.linalg.eig(K)\n",
    "    Lambda[Lambda < 0] = 0\n",
    "    Ks[1].append(np.real(np.dot(V, np.dot(np.diag(Lambda), V.T))))\n",
    "del K\n",
    "del Lambda\n",
    "del V\n",
    "\n",
    "# prepare trees in PTK format\n",
    "import ptk_utils\n",
    "X_ptk = []\n",
    "for x_nodes, x_adj in X:\n",
    "    X_ptk.append(ptk_utils.to_ptk_tree(x_nodes, x_adj))\n",
    "    \n",
    "# pre-compute PTK kernel matrices\n",
    "lambdas = [0.001, 0.01, 0.1]\n",
    "\n",
    "for kernel_type in range(2, len(kernel_types)):\n",
    "    for l in range(len(lambdas)):\n",
    "        Ks[kernel_type].append(ptk_utils.pairwise_kernel_symmetric(X_ptk, kernel_types[kernel_type], lambdas[l]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show kernels\n",
    "for kernel_type in range(2, len(kernel_types)):\n",
    "    fig, axs = plt.subplots()\n",
    "    cax = axs.matshow(Ks[kernel_type][-1])\n",
    "    fig.colorbar(cax)\n",
    "    plt.title(kernel_types[kernel_type])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up out-of-sample methods for each kernel type\n",
    "def oos_linear(x_nodes, x_adj, train_index):\n",
    "    X_train = []\n",
    "    for i in train_index:\n",
    "        X_train.append(X[i])\n",
    "    # compute the edit distances to all training data points\n",
    "    d = mp.pairwise_distances([(x_nodes, x_adj)], X_train, ted.standard_ted)\n",
    "    # compute the kernel values\n",
    "    return 0.5 * (-d + np.mean(d) + np.mean(D[:, train_index], axis=0) - np.mean(D))\n",
    "    \n",
    "def oos_rbf(x_nodes, x_adj, train_index, sigma):\n",
    "    X_train = []\n",
    "    for i in train_index:\n",
    "        X_train.append(X[i])\n",
    "    # compute the edit distances to all training data points\n",
    "    d = mp.pairwise_distances([(x_nodes, x_adj)], X_train, ted.standard_ted)\n",
    "    # compute the kernel values\n",
    "    return np.exp(-0.5 * np.square(d) / (sigma ** 2))\n",
    "    \n",
    "def oos_ptk(x_nodes, x_adj, train_index, kernel_type, lambda_):\n",
    "    x = ptk_utils.to_ptk_tree(x_nodes, x_adj)\n",
    "    X_train = []\n",
    "    for i in train_index:\n",
    "        X_train.append(ptk_utils.to_ptk_tree(X[i][0], X[i][1]))\n",
    "    # compute the edit distances to all training data points\n",
    "    return ptk_utils.pairwise_kernel([x], X_train, kernel_types[kernel_type], lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather the alphabet of out data set for recursive neural net computations\n",
    "neural_net_types = ['recursive', 'tree echo state']\n",
    "\n",
    "alphabet = set()\n",
    "for i in range(len(X)):\n",
    "    alphabet.update(X[i][0])\n",
    "alphabet = list(alphabet)\n",
    "alphabet.sort()\n",
    "dim = 10\n",
    "unique_labels = list(np.unique(Y))\n",
    "\n",
    "# set up training functions for both neural net types\n",
    "def recursive_train(X_train, Y_train):\n",
    "    rec_net = RecursiveNetClassifier(dim, unique_labels, alphabet)\n",
    "    learning_curve = rec_net.fit(X_train, Y_train, print_step = 100)\n",
    "    print('completed training after %d steps' % len(learning_curve))\n",
    "    return rec_net\n",
    "\n",
    "dims = [10, 50, 100]\n",
    "radii = [0.7, 0.9, 1., 1.5, 2]\n",
    "\n",
    "def tree_echo_state_train(X_train, Y_train):\n",
    "    # perform hyper-parameter optimization first\n",
    "    dim, radius, _ = grid_search_cv_echo_state(X_train, Y_train, alphabet, dims, radii)\n",
    "    print('finished hyper parameter optimization')\n",
    "    # train tree echo state network\n",
    "    rec_net = TreeEchoStateNetClassifier(dim, alphabet, radius=radius)\n",
    "    rec_net.fit(X_train, Y_train)\n",
    "    return rec_net\n",
    "\n",
    "neural_net_train_funs = [recursive_train, tree_echo_state_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.7/site-packages/sklearn/utils/__init__.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- fold 1 of 6 ---\n",
      "linear kernel\n",
      "finished hyper parameter optimization\n",
      "accuracy: 1\n",
      "random adversarial accuracy: 0.875\n",
      "adversarial accuracy: 0.375\n",
      "RBF kernel\n",
      "finished hyper parameter optimization\n",
      "accuracy: 1\n",
      "random adversarial accuracy: 0.875\n",
      "adversarial accuracy: 0.375\n",
      "ST kernel\n",
      "finished hyper parameter optimization\n",
      "accuracy: 0.875\n",
      "random adversarial accuracy: 0.142857\n",
      "adversarial accuracy: 0.285714\n",
      "SST kernel\n",
      "finished hyper parameter optimization\n",
      "accuracy: 1\n",
      "random adversarial accuracy: 0.375\n",
      "adversarial accuracy: 0.5\n",
      "PT kernel\n",
      "finished hyper parameter optimization\n",
      "accuracy: 1\n",
      "random adversarial accuracy: 0.25\n",
      "adversarial accuracy: 0.5\n",
      "recursive network\n",
      "lowest loss after 100 steps: 1.86033\n",
      "lowest loss after 200 steps: 1.81279\n",
      "lowest loss after 300 steps: 1.4007\n",
      "lowest loss after 400 steps: 1.24518\n",
      "lowest loss after 500 steps: 1.09026\n",
      "lowest loss after 600 steps: 1.00844\n",
      "lowest loss after 700 steps: 0.948248\n",
      "lowest loss after 800 steps: 0.945099\n",
      "lowest loss after 900 steps: 0.929445\n",
      "lowest loss after 1000 steps: 0.796434\n",
      "lowest loss after 1100 steps: 0.756099\n",
      "lowest loss after 1200 steps: 0.662316\n",
      "lowest loss after 1300 steps: 0.610536\n",
      "lowest loss after 1400 steps: 0.57152\n",
      "lowest loss after 1500 steps: 0.513404\n",
      "lowest loss after 1600 steps: 0.427358\n",
      "lowest loss after 1700 steps: 0.427358\n",
      "lowest loss after 1800 steps: 0.37444\n",
      "lowest loss after 1900 steps: 0.373282\n",
      "lowest loss after 2000 steps: 0.341135\n",
      "lowest loss after 2100 steps: 0.310184\n",
      "lowest loss after 2200 steps: 0.310184\n",
      "lowest loss after 2300 steps: 0.28886\n",
      "lowest loss after 2400 steps: 0.28886\n",
      "lowest loss after 2500 steps: 0.28886\n",
      "lowest loss after 2600 steps: 0.28886\n",
      "lowest loss after 2700 steps: 0.217164\n",
      "lowest loss after 2800 steps: 0.217164\n",
      "lowest loss after 2900 steps: 0.217164\n",
      "lowest loss after 3000 steps: 0.217164\n",
      "lowest loss after 3100 steps: 0.217164\n",
      "lowest loss after 3200 steps: 0.217164\n",
      "lowest loss after 3300 steps: 0.217164\n",
      "lowest loss after 3400 steps: 0.160412\n",
      "lowest loss after 3500 steps: 0.160412\n",
      "lowest loss after 3600 steps: 0.124867\n",
      "lowest loss after 3700 steps: 0.0964334\n",
      "lowest loss after 3800 steps: 0.0887605\n",
      "lowest loss after 3900 steps: 0.0887605\n",
      "lowest loss after 4000 steps: 0.0887605\n",
      "lowest loss after 4100 steps: 0.0887605\n",
      "lowest loss after 4200 steps: 0.0671428\n",
      "lowest loss after 4300 steps: 0.0671428\n",
      "lowest loss after 4400 steps: 0.0671428\n",
      "lowest loss after 4500 steps: 0.0671428\n",
      "lowest loss after 4600 steps: 0.0671428\n",
      "lowest loss after 4700 steps: 0.0671428\n",
      "lowest loss after 4800 steps: 0.046239\n",
      "lowest loss after 4900 steps: 0.046239\n",
      "lowest loss after 5000 steps: 0.046239\n",
      "lowest loss after 5100 steps: 0.046239\n",
      "lowest loss after 5200 steps: 0.046239\n",
      "lowest loss after 5300 steps: 0.046239\n",
      "lowest loss after 5400 steps: 0.046239\n",
      "lowest loss after 5500 steps: 0.046239\n",
      "lowest loss after 5600 steps: 0.046239\n",
      "lowest loss after 5700 steps: 0.046239\n",
      "lowest loss after 5800 steps: 0.046239\n",
      "lowest loss after 5900 steps: 0.046239\n",
      "lowest loss after 6000 steps: 0.046239\n",
      "lowest loss after 6100 steps: 0.0331466\n",
      "lowest loss after 6200 steps: 0.0331466\n",
      "lowest loss after 6300 steps: 0.0331466\n",
      "lowest loss after 6400 steps: 0.0331466\n",
      "lowest loss after 6500 steps: 0.0331466\n",
      "lowest loss after 6600 steps: 0.0331466\n",
      "lowest loss after 6700 steps: 0.0331466\n",
      "lowest loss after 6800 steps: 0.0264785\n",
      "lowest loss after 6900 steps: 0.0264785\n",
      "lowest loss after 7000 steps: 0.0264785\n",
      "lowest loss after 7100 steps: 0.0264785\n",
      "lowest loss after 7200 steps: 0.0264785\n",
      "lowest loss after 7300 steps: 0.0241477\n",
      "lowest loss after 7400 steps: 0.0196404\n",
      "lowest loss after 7500 steps: 0.0196404\n",
      "lowest loss after 7600 steps: 0.0185023\n",
      "lowest loss after 7700 steps: 0.0185023\n",
      "lowest loss after 7800 steps: 0.0182943\n",
      "lowest loss after 7900 steps: 0.0182943\n",
      "lowest loss after 8000 steps: 0.0178164\n",
      "lowest loss after 8100 steps: 0.0166536\n",
      "completed training after 8128 steps\n",
      "accuracy: 1\n",
      "random adversarial accuracy: 0.25\n",
      "adversarial accuracy: 0.25\n",
      "tree echo state network\n",
      "finished hyper parameter optimization\n",
      "accuracy: 0.875\n",
      "random adversarial accuracy: 0.142857\n",
      "adversarial accuracy: 0.285714\n",
      "--- fold 2 of 6 ---\n",
      "linear kernel\n",
      "finished hyper parameter optimization\n",
      "accuracy: 0.875\n",
      "random adversarial accuracy: 0.857143\n",
      "adversarial accuracy: 0.285714\n",
      "RBF kernel\n",
      "finished hyper parameter optimization\n",
      "accuracy: 1\n",
      "random adversarial accuracy: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.7/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/lib64/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adversarial accuracy: 0.5\n",
      "ST kernel\n",
      "finished hyper parameter optimization\n",
      "accuracy: 0.875\n",
      "random adversarial accuracy: 0.285714\n",
      "adversarial accuracy: 0.428571\n",
      "SST kernel\n",
      "finished hyper parameter optimization\n",
      "accuracy: 1\n",
      "random adversarial accuracy: 0.125\n",
      "adversarial accuracy: 0.5\n",
      "PT kernel\n",
      "finished hyper parameter optimization\n",
      "accuracy: 1\n",
      "random adversarial accuracy: 0.125\n",
      "adversarial accuracy: 0.5\n",
      "recursive network\n",
      "lowest loss after 100 steps: 1.95711\n",
      "lowest loss after 200 steps: 1.90634\n",
      "lowest loss after 300 steps: 1.4303\n",
      "lowest loss after 400 steps: 1.25749\n",
      "lowest loss after 500 steps: 1.15083\n",
      "lowest loss after 600 steps: 1.15038\n",
      "lowest loss after 700 steps: 1.11496\n",
      "lowest loss after 800 steps: 1.07781\n",
      "lowest loss after 900 steps: 0.956865\n",
      "lowest loss after 1000 steps: 0.760594\n",
      "lowest loss after 1100 steps: 0.736289\n",
      "lowest loss after 1200 steps: 0.639591\n",
      "lowest loss after 1300 steps: 0.509379\n",
      "lowest loss after 1400 steps: 0.470638\n",
      "lowest loss after 1500 steps: 0.470638\n",
      "lowest loss after 1600 steps: 0.436846\n",
      "lowest loss after 1700 steps: 0.41711\n",
      "lowest loss after 1800 steps: 0.387079\n",
      "lowest loss after 1900 steps: 0.301371\n",
      "lowest loss after 2000 steps: 0.297629\n",
      "lowest loss after 2100 steps: 0.245769\n",
      "lowest loss after 2200 steps: 0.226515\n",
      "lowest loss after 2300 steps: 0.226414\n",
      "lowest loss after 2400 steps: 0.219913\n",
      "lowest loss after 2500 steps: 0.185899\n",
      "lowest loss after 2600 steps: 0.185899\n",
      "lowest loss after 2700 steps: 0.152557\n",
      "lowest loss after 2800 steps: 0.152557\n",
      "lowest loss after 2900 steps: 0.140396\n",
      "lowest loss after 3000 steps: 0.130519\n",
      "lowest loss after 3100 steps: 0.127239\n",
      "lowest loss after 3200 steps: 0.106415\n",
      "lowest loss after 3300 steps: 0.106415\n",
      "lowest loss after 3400 steps: 0.0946675\n",
      "lowest loss after 3500 steps: 0.0850233\n",
      "lowest loss after 3600 steps: 0.0850233\n",
      "lowest loss after 3700 steps: 0.0749718\n",
      "lowest loss after 3800 steps: 0.0716551\n",
      "lowest loss after 3900 steps: 0.0668965\n",
      "lowest loss after 4000 steps: 0.0605971\n",
      "lowest loss after 4100 steps: 0.0583562\n",
      "lowest loss after 4200 steps: 0.0535337\n",
      "lowest loss after 4300 steps: 0.0505898\n",
      "lowest loss after 4400 steps: 0.0505898\n",
      "lowest loss after 4500 steps: 0.0427613\n",
      "lowest loss after 4600 steps: 0.0427613\n",
      "lowest loss after 4700 steps: 0.0427613\n",
      "lowest loss after 4800 steps: 0.0352184\n",
      "lowest loss after 4900 steps: 0.0333496\n",
      "lowest loss after 5000 steps: 0.0330989\n",
      "lowest loss after 5100 steps: 0.0296218\n",
      "lowest loss after 5200 steps: 0.0251263\n",
      "lowest loss after 5300 steps: 0.0251263\n",
      "lowest loss after 5400 steps: 0.0251263\n",
      "lowest loss after 5500 steps: 0.0251263\n",
      "lowest loss after 5600 steps: 0.0209365\n",
      "lowest loss after 5700 steps: 0.0209365\n",
      "lowest loss after 5800 steps: 0.0209365\n",
      "lowest loss after 5900 steps: 0.0202754\n",
      "lowest loss after 6000 steps: 0.0180395\n",
      "lowest loss after 6100 steps: 0.0180395\n",
      "lowest loss after 6200 steps: 0.0172678\n",
      "lowest loss after 6300 steps: 0.0146264\n",
      "lowest loss after 6400 steps: 0.0143085\n",
      "lowest loss after 6500 steps: 0.0143085\n",
      "lowest loss after 6600 steps: 0.013165\n",
      "lowest loss after 6700 steps: 0.013165\n",
      "lowest loss after 6800 steps: 0.0108248\n",
      "lowest loss after 6900 steps: 0.0103922\n",
      "lowest loss after 7000 steps: 0.0103171\n",
      "lowest loss after 7100 steps: 0.0103171\n",
      "lowest loss after 7200 steps: 0.0103171\n",
      "lowest loss after 7300 steps: 0.0103171\n",
      "lowest loss after 7400 steps: 0.0101212\n",
      "lowest loss after 7500 steps: 0.0101212\n",
      "lowest loss after 7600 steps: 0.0101212\n",
      "lowest loss after 7700 steps: 0.0101212\n",
      "completed training after 7741 steps\n",
      "accuracy: 1\n",
      "random adversarial accuracy: 0.375\n",
      "adversarial accuracy: 0.375\n",
      "tree echo state network\n",
      "finished hyper parameter optimization\n",
      "accuracy: 0.875\n",
      "random adversarial accuracy: 0\n",
      "adversarial accuracy: 0.285714\n",
      "--- fold 3 of 6 ---\n",
      "linear kernel\n",
      "finished hyper parameter optimization\n",
      "accuracy: 1\n",
      "random adversarial accuracy: 0.75\n",
      "adversarial accuracy: 0.375\n",
      "RBF kernel\n",
      "finished hyper parameter optimization\n",
      "accuracy: 1\n",
      "random adversarial accuracy: 1\n",
      "adversarial accuracy: 0.25\n",
      "ST kernel\n",
      "finished hyper parameter optimization\n",
      "accuracy: 0.875\n",
      "random adversarial accuracy: 0.142857\n",
      "adversarial accuracy: 0.142857\n",
      "SST kernel\n",
      "finished hyper parameter optimization\n",
      "accuracy: 1\n",
      "random adversarial accuracy: 0.125\n",
      "adversarial accuracy: 0.25\n",
      "PT kernel\n",
      "finished hyper parameter optimization\n",
      "accuracy: 1\n",
      "random adversarial accuracy: 0.25\n",
      "adversarial accuracy: 0.25\n",
      "recursive network\n",
      "lowest loss after 100 steps: 1.85206\n",
      "lowest loss after 200 steps: 1.71431\n",
      "lowest loss after 300 steps: 1.44378\n",
      "lowest loss after 400 steps: 1.15836\n",
      "lowest loss after 500 steps: 1.06678\n",
      "lowest loss after 600 steps: 0.957161\n",
      "lowest loss after 700 steps: 0.957161\n",
      "lowest loss after 800 steps: 0.860259\n",
      "lowest loss after 900 steps: 0.860259\n",
      "lowest loss after 1000 steps: 0.860259\n",
      "lowest loss after 1100 steps: 0.843736\n",
      "lowest loss after 1200 steps: 0.821021\n",
      "lowest loss after 1300 steps: 0.821021\n",
      "lowest loss after 1400 steps: 0.713455\n",
      "lowest loss after 1500 steps: 0.713455\n",
      "lowest loss after 1600 steps: 0.634206\n",
      "lowest loss after 1700 steps: 0.631137\n",
      "lowest loss after 1800 steps: 0.543391\n",
      "lowest loss after 1900 steps: 0.46257\n",
      "lowest loss after 2000 steps: 0.350698\n",
      "lowest loss after 2100 steps: 0.350698\n",
      "lowest loss after 2200 steps: 0.350698\n",
      "lowest loss after 2300 steps: 0.350698\n",
      "lowest loss after 2400 steps: 0.204131\n",
      "lowest loss after 2500 steps: 0.204131\n",
      "lowest loss after 2600 steps: 0.157959\n",
      "lowest loss after 2700 steps: 0.157959\n",
      "lowest loss after 2800 steps: 0.145021\n",
      "lowest loss after 2900 steps: 0.145021\n",
      "lowest loss after 3000 steps: 0.145021\n",
      "lowest loss after 3100 steps: 0.145021\n",
      "lowest loss after 3200 steps: 0.145021\n",
      "lowest loss after 3300 steps: 0.145021\n",
      "lowest loss after 3400 steps: 0.145021\n",
      "lowest loss after 3500 steps: 0.123375\n",
      "lowest loss after 3600 steps: 0.123375\n",
      "lowest loss after 3700 steps: 0.0573249\n",
      "lowest loss after 3800 steps: 0.0573249\n",
      "lowest loss after 3900 steps: 0.0573249\n",
      "lowest loss after 4000 steps: 0.0573249\n",
      "lowest loss after 4100 steps: 0.0573249\n",
      "lowest loss after 4200 steps: 0.055347\n",
      "lowest loss after 4300 steps: 0.055347\n",
      "lowest loss after 4400 steps: 0.055347\n",
      "lowest loss after 4500 steps: 0.0528327\n",
      "lowest loss after 4600 steps: 0.0528327\n",
      "lowest loss after 4700 steps: 0.0398483\n",
      "lowest loss after 4800 steps: 0.0398483\n",
      "lowest loss after 4900 steps: 0.0284147\n",
      "lowest loss after 5000 steps: 0.0284147\n",
      "lowest loss after 5100 steps: 0.0284147\n",
      "lowest loss after 5200 steps: 0.0284147\n",
      "lowest loss after 5300 steps: 0.0284147\n",
      "lowest loss after 5400 steps: 0.0243746\n",
      "lowest loss after 5500 steps: 0.0231475\n",
      "lowest loss after 5600 steps: 0.0231475\n",
      "lowest loss after 5700 steps: 0.0231475\n",
      "lowest loss after 5800 steps: 0.0226871\n",
      "lowest loss after 5900 steps: 0.0199283\n",
      "lowest loss after 6000 steps: 0.0199283\n",
      "lowest loss after 6100 steps: 0.0185137\n",
      "lowest loss after 6200 steps: 0.0170438\n",
      "lowest loss after 6300 steps: 0.0148973\n",
      "lowest loss after 6400 steps: 0.0148973\n",
      "lowest loss after 6500 steps: 0.0138497\n",
      "lowest loss after 6600 steps: 0.0138497\n",
      "lowest loss after 6700 steps: 0.0122995\n",
      "lowest loss after 6800 steps: 0.0118313\n",
      "lowest loss after 6900 steps: 0.0115867\n",
      "lowest loss after 7000 steps: 0.0112101\n",
      "lowest loss after 7100 steps: 0.0102444\n",
      "completed training after 7139 steps\n",
      "accuracy: 0.75\n",
      "random adversarial accuracy: 0.333333\n",
      "adversarial accuracy: 0.166667\n",
      "tree echo state network\n",
      "finished hyper parameter optimization\n",
      "accuracy: 1\n",
      "random adversarial accuracy: 0\n",
      "adversarial accuracy: 0.375\n",
      "--- fold 4 of 6 ---\n",
      "linear kernel\n",
      "finished hyper parameter optimization\n",
      "accuracy: 0.875\n",
      "random adversarial accuracy: 1\n",
      "adversarial accuracy: 0.714286\n",
      "RBF kernel\n",
      "finished hyper parameter optimization\n",
      "accuracy: 1\n",
      "random adversarial accuracy: 0.875\n",
      "adversarial accuracy: 0.75\n",
      "ST kernel\n",
      "finished hyper parameter optimization\n",
      "accuracy: 0.875\n",
      "random adversarial accuracy: 0.142857\n",
      "adversarial accuracy: 0.285714\n",
      "SST kernel\n",
      "finished hyper parameter optimization\n",
      "accuracy: 0.875\n",
      "random adversarial accuracy: 0.428571\n",
      "adversarial accuracy: 0.571429\n",
      "PT kernel\n",
      "finished hyper parameter optimization\n",
      "accuracy: 0.875\n",
      "random adversarial accuracy: 0.285714\n",
      "adversarial accuracy: 0.571429\n",
      "recursive network\n",
      "lowest loss after 100 steps: 1.90463\n",
      "lowest loss after 200 steps: 1.80131\n",
      "lowest loss after 300 steps: 1.44211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowest loss after 400 steps: 1.11417\n",
      "lowest loss after 500 steps: 1.11417\n",
      "lowest loss after 600 steps: 1.09055\n",
      "lowest loss after 700 steps: 1.04761\n",
      "lowest loss after 800 steps: 0.927535\n",
      "lowest loss after 900 steps: 0.769634\n",
      "lowest loss after 1000 steps: 0.710515\n",
      "lowest loss after 1100 steps: 0.687182\n",
      "lowest loss after 1200 steps: 0.673491\n",
      "lowest loss after 1300 steps: 0.602877\n",
      "lowest loss after 1400 steps: 0.555081\n",
      "lowest loss after 1500 steps: 0.471385\n",
      "lowest loss after 1600 steps: 0.436517\n",
      "lowest loss after 1700 steps: 0.436517\n",
      "lowest loss after 1800 steps: 0.328267\n",
      "lowest loss after 1900 steps: 0.197429\n",
      "lowest loss after 2000 steps: 0.197429\n",
      "lowest loss after 2100 steps: 0.197429\n",
      "lowest loss after 2200 steps: 0.197429\n",
      "lowest loss after 2300 steps: 0.197429\n",
      "lowest loss after 2400 steps: 0.162603\n",
      "lowest loss after 2500 steps: 0.11293\n",
      "lowest loss after 2600 steps: 0.104627\n",
      "lowest loss after 2700 steps: 0.104627\n",
      "lowest loss after 2800 steps: 0.0956619\n",
      "lowest loss after 2900 steps: 0.0956619\n",
      "lowest loss after 3000 steps: 0.0956619\n",
      "lowest loss after 3100 steps: 0.0727366\n",
      "lowest loss after 3200 steps: 0.0552443\n",
      "lowest loss after 3300 steps: 0.0521509\n",
      "lowest loss after 3400 steps: 0.0521509\n",
      "lowest loss after 3500 steps: 0.0521509\n",
      "lowest loss after 3600 steps: 0.0415815\n",
      "lowest loss after 3700 steps: 0.0415815\n",
      "lowest loss after 3800 steps: 0.0415815\n",
      "lowest loss after 3900 steps: 0.0415815\n",
      "lowest loss after 4000 steps: 0.0415815\n",
      "lowest loss after 4100 steps: 0.0415815\n",
      "lowest loss after 4200 steps: 0.0256924\n",
      "lowest loss after 4300 steps: 0.0256513\n",
      "lowest loss after 4400 steps: 0.024823\n",
      "lowest loss after 4500 steps: 0.021314\n",
      "lowest loss after 4600 steps: 0.021314\n",
      "lowest loss after 4700 steps: 0.0181464\n",
      "lowest loss after 4800 steps: 0.0181464\n",
      "lowest loss after 4900 steps: 0.0181464\n",
      "lowest loss after 5000 steps: 0.0181015\n",
      "lowest loss after 5100 steps: 0.0164105\n",
      "lowest loss after 5200 steps: 0.0164105\n",
      "lowest loss after 5300 steps: 0.0164105\n",
      "lowest loss after 5400 steps: 0.0164105\n",
      "lowest loss after 5500 steps: 0.0164105\n",
      "lowest loss after 5600 steps: 0.0164105\n",
      "lowest loss after 5700 steps: 0.0164105\n",
      "lowest loss after 5800 steps: 0.0164105\n",
      "lowest loss after 5900 steps: 0.0164105\n",
      "lowest loss after 6000 steps: 0.0164105\n",
      "lowest loss after 6100 steps: 0.0158462\n",
      "lowest loss after 6200 steps: 0.0158462\n",
      "lowest loss after 6300 steps: 0.0158462\n",
      "lowest loss after 6400 steps: 0.0158462\n",
      "lowest loss after 6500 steps: 0.0158462\n",
      "lowest loss after 6600 steps: 0.0158462\n",
      "lowest loss after 6700 steps: 0.0158462\n",
      "lowest loss after 6800 steps: 0.0158462\n",
      "completed training after 6852 steps\n",
      "accuracy: 0.625\n",
      "random adversarial accuracy: 0\n",
      "adversarial accuracy: 0.2\n",
      "tree echo state network\n",
      "finished hyper parameter optimization\n",
      "accuracy: 0.875\n",
      "random adversarial accuracy: 0\n",
      "adversarial accuracy: 0.142857\n",
      "--- fold 5 of 6 ---\n",
      "linear kernel\n",
      "finished hyper parameter optimization\n",
      "accuracy: 1\n",
      "random adversarial accuracy: 1\n",
      "adversarial accuracy: 0.5\n",
      "RBF kernel\n",
      "finished hyper parameter optimization\n",
      "accuracy: 1\n",
      "random adversarial accuracy: 1\n",
      "adversarial accuracy: 0.375\n",
      "ST kernel\n",
      "finished hyper parameter optimization\n",
      "accuracy: 1\n",
      "random adversarial accuracy: 0.125\n",
      "adversarial accuracy: 0.375\n",
      "SST kernel\n",
      "finished hyper parameter optimization\n",
      "accuracy: 1\n",
      "random adversarial accuracy: 0.25\n",
      "adversarial accuracy: 0.375\n",
      "PT kernel\n",
      "finished hyper parameter optimization\n",
      "accuracy: 1\n",
      "random adversarial accuracy: 0.125\n",
      "adversarial accuracy: 0.375\n",
      "recursive network\n",
      "lowest loss after 100 steps: 1.90839\n",
      "lowest loss after 200 steps: 1.89502\n",
      "lowest loss after 300 steps: 1.58988\n",
      "lowest loss after 400 steps: 1.25441\n",
      "lowest loss after 500 steps: 1.12449\n",
      "lowest loss after 600 steps: 0.995711\n",
      "lowest loss after 700 steps: 0.971505\n",
      "lowest loss after 800 steps: 0.884018\n",
      "lowest loss after 900 steps: 0.786081\n",
      "lowest loss after 1000 steps: 0.745883\n",
      "lowest loss after 1100 steps: 0.67202\n",
      "lowest loss after 1200 steps: 0.562909\n",
      "lowest loss after 1300 steps: 0.562909\n",
      "lowest loss after 1400 steps: 0.484657\n",
      "lowest loss after 1500 steps: 0.469511\n",
      "lowest loss after 1600 steps: 0.419367\n",
      "lowest loss after 1700 steps: 0.351248\n",
      "lowest loss after 1800 steps: 0.275907\n",
      "lowest loss after 1900 steps: 0.275907\n",
      "lowest loss after 2000 steps: 0.236783\n",
      "lowest loss after 2100 steps: 0.197486\n",
      "lowest loss after 2200 steps: 0.159806\n",
      "lowest loss after 2300 steps: 0.139583\n",
      "lowest loss after 2400 steps: 0.139583\n",
      "lowest loss after 2500 steps: 0.129944\n",
      "lowest loss after 2600 steps: 0.114286\n",
      "lowest loss after 2700 steps: 0.112341\n",
      "lowest loss after 2800 steps: 0.0872048\n",
      "lowest loss after 2900 steps: 0.0852159\n",
      "lowest loss after 3000 steps: 0.0769779\n",
      "lowest loss after 3100 steps: 0.0655129\n",
      "lowest loss after 3200 steps: 0.0644382\n",
      "lowest loss after 3300 steps: 0.0538042\n",
      "lowest loss after 3400 steps: 0.0486425\n",
      "lowest loss after 3500 steps: 0.0473825\n",
      "lowest loss after 3600 steps: 0.0416106\n",
      "lowest loss after 3700 steps: 0.0373631\n",
      "lowest loss after 3800 steps: 0.0346589\n",
      "lowest loss after 3900 steps: 0.0333726\n",
      "lowest loss after 4000 steps: 0.0296706\n",
      "lowest loss after 4100 steps: 0.0268632\n",
      "lowest loss after 4200 steps: 0.0264958\n",
      "lowest loss after 4300 steps: 0.024765\n",
      "lowest loss after 4400 steps: 0.0215365\n",
      "lowest loss after 4500 steps: 0.0197289\n",
      "lowest loss after 4600 steps: 0.0174012\n",
      "lowest loss after 4700 steps: 0.0166904\n",
      "lowest loss after 4800 steps: 0.0164177\n",
      "lowest loss after 4900 steps: 0.0153476\n",
      "lowest loss after 5000 steps: 0.0149009\n",
      "lowest loss after 5100 steps: 0.0133733\n",
      "lowest loss after 5200 steps: 0.0127602\n",
      "lowest loss after 5300 steps: 0.0120828\n",
      "lowest loss after 5400 steps: 0.0104584\n",
      "lowest loss after 5500 steps: 0.0104584\n",
      "completed training after 5579 steps\n",
      "accuracy: 0.875\n",
      "random adversarial accuracy: 0.428571\n",
      "adversarial accuracy: 0.142857\n",
      "tree echo state network\n",
      "finished hyper parameter optimization\n",
      "accuracy: 1\n",
      "random adversarial accuracy: 0\n",
      "adversarial accuracy: 0.25\n",
      "--- fold 6 of 6 ---\n",
      "linear kernel\n",
      "finished hyper parameter optimization\n",
      "accuracy: 1\n",
      "random adversarial accuracy: 1\n",
      "adversarial accuracy: 0.625\n",
      "RBF kernel\n",
      "finished hyper parameter optimization\n",
      "accuracy: 1\n",
      "random adversarial accuracy: 0.875\n",
      "adversarial accuracy: 0.625\n",
      "ST kernel\n",
      "finished hyper parameter optimization\n",
      "accuracy: 0.75\n",
      "random adversarial accuracy: 0\n",
      "adversarial accuracy: 0.166667\n",
      "SST kernel\n",
      "finished hyper parameter optimization\n",
      "accuracy: 0.875\n",
      "random adversarial accuracy: 0\n",
      "adversarial accuracy: 0.571429\n",
      "PT kernel\n",
      "finished hyper parameter optimization\n",
      "accuracy: 0.875\n",
      "random adversarial accuracy: 0.142857\n",
      "adversarial accuracy: 0.571429\n",
      "recursive network\n",
      "lowest loss after 100 steps: 1.95425\n",
      "lowest loss after 200 steps: 1.7275\n",
      "lowest loss after 300 steps: 1.19559\n",
      "lowest loss after 400 steps: 1.01943\n",
      "lowest loss after 500 steps: 0.969234\n",
      "lowest loss after 600 steps: 0.875667\n",
      "lowest loss after 700 steps: 0.70231\n",
      "lowest loss after 800 steps: 0.656464\n",
      "lowest loss after 900 steps: 0.604284\n",
      "lowest loss after 1000 steps: 0.514202\n",
      "lowest loss after 1100 steps: 0.492021\n",
      "lowest loss after 1200 steps: 0.370439\n",
      "lowest loss after 1300 steps: 0.370439\n",
      "lowest loss after 1400 steps: 0.321034\n",
      "lowest loss after 1500 steps: 0.22724\n",
      "lowest loss after 1600 steps: 0.22724\n",
      "lowest loss after 1700 steps: 0.184081\n",
      "lowest loss after 1800 steps: 0.160687\n",
      "lowest loss after 1900 steps: 0.15675\n",
      "lowest loss after 2000 steps: 0.15675\n",
      "lowest loss after 2100 steps: 0.13684\n",
      "lowest loss after 2200 steps: 0.117275\n",
      "lowest loss after 2300 steps: 0.108248\n",
      "lowest loss after 2400 steps: 0.0869139\n",
      "lowest loss after 2500 steps: 0.0820795\n",
      "lowest loss after 2600 steps: 0.0784794\n",
      "lowest loss after 2700 steps: 0.072271\n",
      "lowest loss after 2800 steps: 0.0634295\n",
      "lowest loss after 2900 steps: 0.0585298\n",
      "lowest loss after 3000 steps: 0.0521914\n",
      "lowest loss after 3100 steps: 0.0493175\n",
      "lowest loss after 3200 steps: 0.0423626\n",
      "lowest loss after 3300 steps: 0.0369117\n",
      "lowest loss after 3400 steps: 0.0369117\n",
      "lowest loss after 3500 steps: 0.0369117\n",
      "lowest loss after 3600 steps: 0.0310643\n",
      "lowest loss after 3700 steps: 0.029679\n",
      "lowest loss after 3800 steps: 0.0252612\n",
      "lowest loss after 3900 steps: 0.0230428\n",
      "lowest loss after 4000 steps: 0.0230428\n",
      "lowest loss after 4100 steps: 0.0198695\n",
      "lowest loss after 4200 steps: 0.0198695\n",
      "lowest loss after 4300 steps: 0.0198455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowest loss after 4400 steps: 0.0182426\n",
      "lowest loss after 4500 steps: 0.0148477\n",
      "lowest loss after 4600 steps: 0.0148477\n",
      "lowest loss after 4700 steps: 0.0135227\n",
      "lowest loss after 4800 steps: 0.0132329\n",
      "lowest loss after 4900 steps: 0.0107851\n",
      "lowest loss after 5000 steps: 0.0107851\n",
      "lowest loss after 5100 steps: 0.0107851\n",
      "completed training after 5113 steps\n",
      "accuracy: 0.875\n",
      "random adversarial accuracy: 0.285714\n",
      "adversarial accuracy: 0.142857\n",
      "tree echo state network\n",
      "finished hyper parameter optimization\n",
      "accuracy: 0.875\n",
      "random adversarial accuracy: 0.142857\n",
      "adversarial accuracy: 0.428571\n"
     ]
    }
   ],
   "source": [
    "# set up crossvalidation folds\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from recursive_net import RecursiveNetClassifier\n",
    "from tree_echo_state import TreeEchoStateNetClassifier\n",
    "from hyperopt import grid_search_cv_svm\n",
    "from hyperopt import grid_search_cv_echo_state\n",
    "from adversarial_edits import construct_random_adversarials\n",
    "from adversarial_edits import construct_adversarials\n",
    "\n",
    "n_split_outer = 6\n",
    "n_split_inner = 5\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=n_split_outer, shuffle=True)\n",
    "\n",
    "# set up result matrices\n",
    "conditions = ['original', 'random', 'adversarial']\n",
    "\n",
    "accs = np.zeros((n_split_outer, len(kernel_types) + len(neural_net_types), len(conditions)))\n",
    "d_adversarial = np.zeros((n_split_outer, len(kernel_types) + len(neural_net_types), len(conditions) - 1))\n",
    "Zs = [[[None] * len(X)] * (len(kernel_types) + len(neural_net_types))] * (len(conditions) - 1)\n",
    "\n",
    "# iterate over the folds\n",
    "f = -1\n",
    "for train_index, test_index in outer_cv.split(X, Y):\n",
    "    f += 1\n",
    "    print('--- fold %d of %d ---' % (f+1, n_split_outer))\n",
    "    X_train = []\n",
    "    for i in train_index:\n",
    "        X_train.append(X[i])\n",
    "    X_test = []\n",
    "    for i in test_index:\n",
    "        X_test.append(X[i])\n",
    "\n",
    "    # iterate over our kernels\n",
    "    for kernel_type in range(len(kernel_types)):\n",
    "        print('%s kernel' % kernel_types[kernel_type])\n",
    "        # perform a nested crossvalidation to optimize hyper-parameters\n",
    "        C, k, _ = grid_search_cv_svm(Y, Cs, Ks[kernel_type], n_split_inner, train_index)\n",
    "        print('finished hyper parameter optimization')\n",
    "        # select the best kernel matrix\n",
    "        K = Ks[kernel_type][k]\n",
    "        K_train = K[train_index, :][:, train_index]\n",
    "        K_test  = K[test_index, :][:, train_index]\n",
    "        # train an SVM with the optimal hyperparameters\n",
    "        svm = SVC(kernel='precomputed', decision_function_shape='ovo', C = C)\n",
    "        svm.fit(K_train, Y[train_index])\n",
    "        # evaluate the test accuracy\n",
    "        Y_pred  = svm.predict(K_test)\n",
    "        correct = Y[test_index] == Y_pred\n",
    "        accs[f, kernel_type, 0] = accuracy_score(Y[test_index], Y_pred)\n",
    "        print('accuracy: %g' % accs[f, kernel_type, 0])\n",
    "        # then, start the adversarial construction on the test data\n",
    "        # set up the out of sample function for the current kernel\n",
    "        if(kernel_type == 0):\n",
    "            oos = oos_linear\n",
    "        elif(kernel_type == 1):\n",
    "            oos = lambda x_nodes, x_adj, train_index: oos_rbf(x_nodes, x_adj, train_index, sigmas[k])\n",
    "        else:\n",
    "            oos = lambda x_nodes, x_adj, train_index: oos_ptk(x_nodes, x_adj, train_index, kernel_type, lambdas[k])\n",
    "        # set up the predictive function for the current kernel\n",
    "        def predict(x_nodes, x_adj):\n",
    "            # compute the kernel values of the new point to the trainign data\n",
    "            k = oos(x_nodes, x_adj, train_index)\n",
    "            # and then predict with the SVM\n",
    "            return svm.predict(k)\n",
    "        # construct random adversarials\n",
    "        Z, labels, ds = construct_random_adversarials(X_test, Y[test_index], Y_pred, predict, alphabet, max_d = 100)\n",
    "        # verify labels\n",
    "        success = np.logical_not(np.isinf(ds))\n",
    "        if(np.any(labels[success] == Y[test_index][success])):\n",
    "            raise ValueError('expected changed labels in all successful cases')\n",
    "        # store the proportion of attempts where adversarial construction failed\n",
    "        accs[f, kernel_type, 1] = np.mean(ds[correct] >= 1.)\n",
    "        print('random adversarial accuracy: %g' % accs[f, kernel_type, 1])\n",
    "        # the average relative distance to an adversarial example\n",
    "        d_adversarial[f, kernel_type, 0] = np.mean(ds[success])\n",
    "        # and the adversarial examples themselves\n",
    "        for i in range(len(test_index)):\n",
    "            Zs[0][kernel_type][test_index[i]] = Z[i]\n",
    "        # construct adversarials\n",
    "        Z, labels, ds = construct_adversarials(X_test, D[test_index, :][:, test_index], Y[test_index], Y_pred, predict)\n",
    "        # verify labels\n",
    "        success = np.logical_not(np.isinf(ds))\n",
    "        if(np.any(labels[success] == Y[test_index][success])):\n",
    "            raise ValueError('expected changed labels in all successful cases')\n",
    "        # store the proportion of attempts where adversarial construction failed\n",
    "        accs[f, kernel_type, 2] = np.mean(ds[correct] >= 1.)\n",
    "        print('adversarial accuracy: %g' % accs[f, kernel_type, 2])\n",
    "        # the average relative distance to an adversarial example\n",
    "        d_adversarial[f, kernel_type, 1] = np.mean(ds[np.logical_not(np.isinf(ds))])\n",
    "        # and the adversarial examples themselves\n",
    "        for i in range(len(test_index)):\n",
    "            Zs[1][kernel_type][test_index[i]] = Z[i]\n",
    "    # also train the neural nets on this data\n",
    "    for net_type in range(len(neural_net_types)):\n",
    "        t = len(kernel_types) + net_type\n",
    "        print('%s network' % neural_net_types[net_type])\n",
    "        rec_net = neural_net_train_funs[net_type](X_train, Y[train_index])\n",
    "        # check the accuracy\n",
    "        Y_pred = np.array(rec_net.predict(X_test))\n",
    "        correct = Y[test_index] == Y_pred\n",
    "        accs[f, t, 0] = accuracy_score(Y[test_index], Y_pred)\n",
    "        print('accuracy: %g' % accs[f, t, 0])\n",
    "        # set up the predictive function\n",
    "        def predict(x_nodes, x_adj):\n",
    "            return rec_net.predict([(x_nodes, x_adj)])[0]\n",
    "        # construct random adversarials\n",
    "        Z, labels, ds = construct_random_adversarials(X_test, Y[test_index], Y_pred, predict, alphabet, max_d = 100)\n",
    "        # verify labels\n",
    "        success = np.logical_not(np.isinf(ds))\n",
    "        if(np.any(labels[success] == Y[test_index][success])):\n",
    "            raise ValueError('expected changed labels in all successful cases')\n",
    "        # store the proportion of attempts where adversarial construction failed\n",
    "        accs[f, t, 1] = np.mean(ds[correct] >= 1.)\n",
    "        print('random adversarial accuracy: %g' % accs[f, t, 1])\n",
    "        # the average relative distance to an adversarial example\n",
    "        d_adversarial[f, t, 0] = np.mean(ds[success])\n",
    "        # and the adversarial examples themselves\n",
    "        for i in range(len(test_index)):\n",
    "            Zs[0][t][test_index[i]] = Z[i]\n",
    "        # construct adversarials\n",
    "        Z, labels, ds = construct_adversarials(X_test, D[test_index, :][:, test_index], Y[test_index], Y_pred, predict)\n",
    "        # verify labels\n",
    "        success = np.logical_not(np.isinf(ds))\n",
    "        if(np.any(labels[success] == Y[test_index][success])):\n",
    "            raise ValueError('expected changed labels in all successful cases')\n",
    "        # store the proportion of attempts where adversarial construction failed\n",
    "        accs[f, t, 2] = np.mean(ds[correct] >= 1.)\n",
    "        print('adversarial accuracy: %g' % accs[f, t, 2])\n",
    "        # the average relative distnace to an adversarial example\n",
    "        d_adversarial[f, t, 1] = np.mean(ds[success])\n",
    "        # and the adversarial examples themselves\n",
    "        for i in range(len(test_index)):\n",
    "            Zs[1][t][test_index[i]] = Z[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear kernel\n",
      "on average, we needed the following distance to flip the label: 2.68068\n",
      "we could generate adversarial examples for 52.0833% of the data\n",
      "RBF kernel\n",
      "on average, we needed the following distance to flip the label: 1.43975\n",
      "we could generate adversarial examples for 52.0833% of the data\n",
      "ST kernel\n",
      "on average, we needed the following distance to flip the label: 0.933978\n",
      "we could generate adversarial examples for 71.9246% of the data\n",
      "SST kernel\n",
      "on average, we needed the following distance to flip the label: 1.90993\n",
      "we could generate adversarial examples for 53.869% of the data\n",
      "PT kernel\n",
      "on average, we needed the following distance to flip the label: 1.90993\n",
      "we could generate adversarial examples for 53.869% of the data\n",
      "recursive network\n",
      "on average, we needed the following distance to flip the label: 1.25805\n",
      "we could generate adversarial examples for 78.7103% of the data\n",
      "tree echo state network\n",
      "on average, we needed the following distance to flip the label: 1.57397\n",
      "we could generate adversarial examples for 70.5357% of the data\n"
     ]
    }
   ],
   "source": [
    "for kernel_type in range(len(kernel_types)):\n",
    "    print('%s kernel' % kernel_types[kernel_type])\n",
    "    print('on average, we needed the following distance to flip the label: %g' % np.mean(d_adversarial[:, kernel_type, -1]))\n",
    "    print('we could generate adversarial examples for %g%% of the data' % (100. - 100.* np.mean(accs[:, kernel_type, -1])))\n",
    "for net_type in range(len(neural_net_types)):\n",
    "    t = len(kernel_types) + net_type\n",
    "    print('%s network' % neural_net_types[net_type])\n",
    "    print('on average, we needed the following distance to flip the label: %g' % np.mean(d_adversarial[:, t, -1]))\n",
    "    print('we could generate adversarial examples for %g%% of the data' % (100. - 100.* np.mean(accs[:, t, -1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the results to a CSV table\n",
    "np.savetxt('results/%s_accs.csv' % dataset_name, accs[:, :, 0], delimiter='\\t', header='\\t'.join(kernel_types + neural_net_types), fmt='%g')\n",
    "np.savetxt('results/%s_accs_random_adversarial.csv' % dataset_name, accs[:, :, 1], delimiter='\\t', header='\\t'.join(kernel_types + neural_net_types), fmt='%g')\n",
    "np.savetxt('results/%s_d_random_adversarial.csv' % dataset_name, d_adversarial[:, :, 0], delimiter='\\t', header='\\t'.join(kernel_types + neural_net_types), fmt='%g')\n",
    "np.savetxt('results/%s_accs_adversarial.csv' % dataset_name, accs[:, :, 2], delimiter='\\t', header='\\t'.join(kernel_types + neural_net_types), fmt='%g')\n",
    "np.savetxt('results/%s_d_adversarial.csv' % dataset_name, d_adversarial[:, :, 1], delimiter='\\t', header='\\t'.join(kernel_types + neural_net_types), fmt='%g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the adversarial examples to a pickle file\n",
    "import pickle\n",
    "with open('results/%s_adversarials.pickle' % dataset_name, 'wb') as pickle_file:\n",
    "    pickle.dump(Zs, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
